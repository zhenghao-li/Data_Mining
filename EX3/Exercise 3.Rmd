---
title: "Exercise 3"
author: By Eliza Malinova, Zhenghao Li, and Raushan Baizakova
output: github_document 
always_allow_html: yes
--- 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning=FALSE)
library(tidyverse)
library(mosaic)
library(MASS) 
library(tidyverse)
library(FNN)
library(dplyr)
library(gamlr) #for Lasso
library(glmnet) #for Lasso
library(ggplot2)
library(randomForest) #for both bagging and random forests
library(kableExtra)
library(data.table)
library(sjPlot)
library(sjmisc)
library(sjlabelled)
library(LICORS) 
library(foreach)
library(cluster)
library(factoextra)
library(knitr)
library(GGally)
library(scales)
library(lubridate)
library(gridExtra)
library(grid)
library(lattice)
```

# Problem 1. Predictive Model Building

To build the most accurate predictive model possible, we compared multiple statistical methods: a hand-built linear model, a linear model using forward selection, Lasso model, and two tree-based models using bagging and random forest, respectively. We first compared the two linear models with Lasso by using train/test splits with a loop of 100 to calculate the RMSE, and we concluded that Lasso did not result in an improved prediction accuracy. Therefore, we included the tree-based models to try and improve our predictions. In order to decide which model performs best, out of the 5 previously described models, we employed Leave-one-out cross validation (LOOCV) RMSE, and chose the one that decreased the RMSE the most. 

Before starting fitting regression models, we needed to omit all missing variables in the dataset. Total observations got reduced from 7,894 to 7,820. In addition, we included green_ratings instead of  LEED and EnergyStar in all of the models, and used cd_total_07 and hd_total07 separately instead of total_dd_07. Since cluster rent is most likely a function of Rent, we omitted it to avoid regressing the target variable on a variable that is a part of the target variable itself. 

### First Model: Hand-Built Linear Model

We built a multiple linear regression model with variables that, based on our knowledge and intuition, can affect rent. The model and the selected variables are shown below: 

Rent<sub>&beta;</sub> = &beta;<sub>1</sub> cluster + &beta;<sub>2</sub> size + &beta;<sub>3</sub> empl_gr + &beta;<sub>4</sub> leasing_rate + &beta;<sub>5</sub> stories + &beta;<sub>6</sub> stories* size + &beta;<sub>7</sub> cd_total_07* Electricity_Costs + &beta;<sub>8</sub> age + &beta;<sub>9</sub> renovated + &beta;<sub>10</sub> class_a + &beta;<sub>11</sub> class_b + &beta;<sub>12</sub> green_rating + &beta;<sub>13</sub> net + &beta;<sub>14</sub> amenities + &beta;<sub>15</sub> cd_total_07 + &beta;<sub>16</sub> hd_total07 + &beta;<sub>17 </sub> Precipitation + &beta;<sub>18</sub> Gas_Costs + &beta;<sub>19</sub> Electricity_Costs + &beta;<sub>20</sub> Electricity_Costs*hd_total07


```{r setup 1.1, results='hide', warning=FALSE, echo=FALSE}
greenbuildings <- read.csv("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/greenbuildings.csv")

greenbuildingsNoNA <- na.omit(greenbuildings) #exclude all rows with omitted and N/A variables; reduced the data set from 8794 to 7820

lm_RentPrice = lm(Rent ~ cluster + size + empl_gr  + leasing_rate 
                  + stories + stories*size + cd_total_07*Electricity_Costs
                  + age + renovated + class_a + class_b + green_rating + net 
                  + amenities + cd_total_07 + hd_total07 + Precipitation 
                  + Gas_Costs + Electricity_Costs + Electricity_Costs*hd_total07 
                  , data=greenbuildingsNoNA)

lm0 = lm(Rent ~ 1, data=greenbuildingsNoNA)
lm_forward = step(lm0, direction='forward',
                  scope=~(cluster + size + empl_gr  + leasing_rate + stories
                          + age + renovated + class_a + class_b + green_rating + net 
                          + amenities + cd_total_07 + hd_total07 + Precipitation 
                          + Gas_Costs + Electricity_Costs)^2)
lm_Rent_Forward = update(lm_forward, data = greenbuildingsNoNA)

tab_model(lm_RentPrice,lm_Rent_Forward, show.ci = FALSE, dv.labels =  c("Hand-Built Linear Model", "Forward Selection Linear Model"))

```

### Second Model: Model Based on Forward Selection

For the second model, we used forward selection in order to select variables that improve the prediction accuracy.

The table below lists the variables and estimates for the hand-built multiple linear regression model and for the forward selected regression model, respectively. 

```{r setup 1.2, warning=FALSE, echo=FALSE}
greenbuildings <- read.csv("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/greenbuildings.csv")

greenbuildingsNoNA <- na.omit(greenbuildings) #exclude all rows with omitted and N/A variables; reduced the data set from 8794 to 7820

tab_model(lm_RentPrice,lm_Rent_Forward, show.ci = FALSE, 
          dv.labels =  c("Hand-Built Linear Model", "Forward Selection Linear Model"),
          title = "Table 1.0: Coefficients of the Hand-Built and Forward Selection Linear Models",
          CSS = list(
            css.depvarhead = 'color: red;',css.summary = 'color: blue;'))

```

### Third Model: Lasso Model

By using a shrinkage method like Lasso, we depart from optimality and concentrate on stabilizing the system. Since the two linear models above include many variables, there is the possibility of high variance because each nonzero independent variable included in the model incurs a cost of employing data to estimate it, hence, increasing the variance. Through Lasso model, we make the cost we incur from having many variables explicit. Therefore, through coefficient shrinkage, we can reduce their variance. 

The variable regression and the accuracy of the Lasso model fit largely depend on the choice of lambda. The coefficient plot below shows that depending on the value of the tuning parameter, some of the coefficients will be set equal to 0. 

```{r setup 1.3, warning=FALSE, echo=FALSE}
greenbuildings <- read.csv("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/greenbuildings.csv")

greenbuildingsNoNA <- na.omit(greenbuildings)

x = model.matrix(Rent ~ cluster + size + empl_gr  + leasing_rate + stories
                 + age + renovated + class_a + class_b + green_rating + net 
                 + amenities + cd_total_07 + hd_total07 + Precipitation 
                 + Gas_Costs + Electricity_Costs, data=greenbuildingsNoNA)[,-1]
x = scale(x, center=TRUE, scale=TRUE) 
y = greenbuildingsNoNA$Rent
grid=10^seq(10,-2, length =100)
lasso.mod=glmnet(x,y,alpha=1, lambda =grid)
cv.out=cv.glmnet(x,y,alpha=1)

bestlam =cv.out$lambda.min

plot(lasso.mod)
title("Figure 1.1: Lasso Coefficients as a Function of L1 Norm", line = 3)
```

By employing Lasso model, we consider the variance-bias trade off, and examine lambda. If lambda is high, the variance decreases but bias increases.

We used cross-validation as a method to select the tuning parameter, lambda. We chose a grid of lambda values that vary from 10^(-2) to 10^10 and computed the leave-one-out cross validation error for each of these values. The tuning parameter of 0.008 has the smallest cross validation error, so we chose it for the refit of the Lasso model. The graph below illustrates the lambda with the least CV Mean Squared Error(MSE). However, the dip of the CV MSE, where the lambda gives a small error, is not very pronounced, resulting in a wide range of values for lambda that will give similar errors. 

```{r setup 1.4, warning=FALSE, echo=FALSE}
greenbuildings <- read.csv("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/greenbuildings.csv")

greenbuildingsNoNA <- na.omit(greenbuildings)

plot(cv.out)
title("Figure 1.2: Mean-Squared Error as a Function of log Lambda", line = 3)
```

Since lambda is very close to 0, approximately 0.008, Lasso’s results will be very close to the least squares models with high variance and low bias. The table below shows the Lasso coefficient estimates. Using lambda  = 0.008, all the 17 coefficient estimates are nonzero. 

```{r setup 1.5, warning=FALSE, echo=FALSE}
greenbuildings <- read.csv("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/greenbuildings.csv")

greenbuildingsNoNA <- na.omit(greenbuildings)
lasso.coef=predict(lasso.mod ,type ="coefficients",s=bestlam)
LassoCoef=as.data.table(as.matrix(lasso.coef), keep.rownames = TRUE)
kable(LassoCoef, col.names = c("Predictor", "Estimate"), caption = "**Table 1.1 Lasso Model Predictor Estimates**",  format_caption = c("italic", "underline")) %>%
  kable_styling(bootstrap_options = "striped", full_width = F)

```

Next, we use train/test splits with a loop of 100 to calculate the average RMSE of each of the three models shown above. Table 1.2 illustrates the average RMSE for each of the models. 

```{r setup 1.6, warning=FALSE, echo=FALSE}

greenbuildings <- read.csv("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/greenbuildings.csv")

greenbuildingsNoNA <- na.omit(greenbuildings)

rmse = function(y, yhat) {
  sqrt(mean((y - yhat)^2))}

LoopModels = do(100)*{
  n = nrow(greenbuildingsNoNA)
  n_train = round(0.8*n)  # round to nearest integer
  n_test = n - n_train
  train_cases = sample.int(n, n_train, replace=FALSE)
  test_cases = setdiff(1:n, train_cases) 
  greenbuildings_train = greenbuildingsNoNA[train_cases,]
  greenbuildings_test = greenbuildingsNoNA[test_cases,]
  
  lm_RentPrice = lm(Rent ~ cluster + size + empl_gr  + leasing_rate 
                + stories + stories*size + cd_total_07*Electricity_Costs
                + age + renovated + class_a + class_b + green_rating + net 
                + amenities + cd_total_07 + hd_total07 + Precipitation 
                + Gas_Costs + Electricity_Costs + Electricity_Costs*hd_total07 
                , data=greenbuildings_train)
  
  lm_Rent_Forward = update(lm_forward, data = greenbuildings_train)
  
  x = model.matrix(Rent ~ cluster + size + empl_gr  + leasing_rate + stories
                   + age + renovated + class_a + class_b + green_rating + net 
                   + amenities + cd_total_07 + hd_total07 + Precipitation 
                   + Gas_Costs + Electricity_Costs, data=greenbuildingsNoNA)[,-1]
  x = scale(x, center=TRUE, scale=TRUE) 
  y = greenbuildingsNoNA$Rent
  grid=10^seq(10,-2, length =100)
  lasso.mod=glmnet(x[train_cases,],y[train_cases],alpha=1, lambda =grid)
  cv.out=cv.glmnet(x[train_cases,],y[train_cases],alpha=1)
  bestlam =cv.out$lambda.min
  
  yhat_test_lmRentPrice = predict(lm_RentPrice, greenbuildings_test)
  yhat_test_RentForward = predict(lm_Rent_Forward, greenbuildings_test)
  yhat_lm_Rent_Lasso = predict(lasso.mod, s=bestlam, newx=x[test_cases,])
  
  c(RMSERentPrice = rmse(greenbuildings_test$Rent, yhat_test_lmRentPrice),
    RMSERentForward = rmse(greenbuildings_test$Rent, yhat_test_RentForward),
    RMSERentLasso = rmse(greenbuildings_test$Rent,yhat_lm_Rent_Lasso))
  
}

RMSEMeans = c("Hand-Built Linear Model" = mean(LoopModels$RMSERentPrice), 
                                   "Forward Selection Linear Model" = mean(LoopModels$RMSERentForward), 
                                   "Lasso" = mean(LoopModels$RMSERentLasso))

kable(RMSEMeans, col.names = c("Average RMSE"), caption = "**Table 1.2 Average RMSE per Model**",  format_caption = c("italic", "underline")) %>%
  kable_styling(bootstrap_options = "striped", full_width = F)

```

In this situation, the lasso regression does not give more accurate predictions than the OLS based models as seen from the RMSE. In addition, the optimal lambda is 0.008, almost 0, therefore, the coefficient estimates are very similar to the least squared regression models. Using both Lasso and OLS results in high variance and low bias. 

### Tree-Based Models

Since Lasso model results in higher RMSE than the linear regression models, we decided to perform two tree decision models using bagging and random forest. 

**Bagging**
 
The reason why we are employing the bagging procedure is because fitting individual decision trees results in high variance. For example, if we decide to use different parts of the data, the decision tree of one part of data split will give outcomes that are very different from the decision tree of another part of the data. Since bagging uses the bootstrap procedure to take repeated samples from the training set and average the resulting prediction, it reduces the variance of the statistical model and therefore increases the prediction accuracy.

The bagging procedure has created 500 trees by bootstrapping, and all the 17 variables are considered at each split since the trees are not pruned. Because all variables have been tried at each split, each individual tree will have high variance and low bias. However, averaging them reduces the variance.

```{r setup 1.7, warning=FALSE, echo=FALSE}

greenbuildings <- read.csv("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/greenbuildings.csv")

greenbuildingsNoNA <- na.omit(greenbuildings)

set.seed(1)
greenbuildBagging = randomForest(Rent ~ cluster + size + empl_gr + leasing_rate + stories + age + renovated + class_a + class_b + green_rating + net + amenities + cd_total_07 + hd_total07 + Precipitation + Gas_Costs + Electricity_Costs, data = greenbuildings_train, mtry=17, importance=TRUE)

greenbuildBagging
```

The plot below shows that the bagging procedure can produce quite accurate predictions most of the time.  

```{r setup 1.8, warning=FALSE, echo=FALSE}

greenbuildings <- read.csv("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/greenbuildings.csv")

greenbuildingsNoNA <- na.omit(greenbuildings)

yhat_greenbuildBagging = predict(greenbuildBagging, newdata = greenbuildings_test)
plot(yhat_greenbuildBagging, greenbuildings_test$Rent, xlab = "Predicted Values for Rent: Bagging", ylab = "Rent")
title("Figure 1.3: Comparison between Bagging Predicted Values of Rent\n and Actual Rent Prices")

```

**Random Forest** 

We also ran a random forest model since it is an improvement procedure over bagging. Random forest also builds decision trees by bootstrapping the training samples. We again used 500 number of trees to be bootstrapped. However, instead of considering all variables in each split, random forest chooses only a set of these variables for the tree split. We decided to use a square root of the number of the original variables, which is sqrt(17), approximately 4 variables. Therefore, at each split, a new sample of these 4 predictors is taken. 

By using a different sample of 4 predictors in each split instead of all 17 variables, we avoid the highly correlated predictions that result from bagging. Random forest produced less correlated predictions. In the bagging procedure, since all variables are used, majority of trees will use the strong predictor at the top of tree, which will result in increased correlation of the predictions. Hence, bagging will usually not lead to a smaller reduction in variance than random forest because it averages highly correlated predictions. Therefore, by averaging uncorrelated quantities, random forest will generally result in a higher decrease in variance in the average of the resulting trees. 

The plot below illustrates the accuracy of the random forest prediction model. 

```{r setup 1.9, warning=FALSE, echo=FALSE}

greenbuildings <- read.csv("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/greenbuildings.csv")

greenbuildingsNoNA <- na.omit(greenbuildings)

set.seed(1)
greenbuildRandomForest = randomForest(Rent ~ cluster + size + empl_gr  + leasing_rate + stories
                                      + age + renovated + class_a + class_b + green_rating + net 
                                      + amenities + cd_total_07 + hd_total07 + Precipitation 
                                      + Gas_Costs + Electricity_Costs, data = greenbuildings_train,
                                      mtry=4, importance=TRUE)
yhat_greenbuildRandomForest = predict(greenbuildRandomForest, newdata = greenbuildings_test)

plot(yhat_greenbuildRandomForest, greenbuildings_test$Rent, xlab = "Predicted Values for Rent: Random Forest", ylab = "Rent")
title("Figure 1.4: Comparison between Random Forest Predicted Values of Rent\n and Actual Rent Prices")
```

### Comparison of all 5 Predictive Models: Hand-Built Linear Model, Forward Selection, Lasso, Bagging and Random Forest

To compare all five models run so far, we are using a cross validation performance measure: the Leave-one-out cross validation (LOOCV). Table 1.3 below lists each RMSE found from performing the CV. 

```{r setup 1.10, warning=FALSE, echo=FALSE}

greenbuildings <- read.csv("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/greenbuildings.csv")

greenbuildingsNoNA <- na.omit(greenbuildings)

N=nrow(greenbuildingsNoNA)
K=5
fold_id = rep_len(1:K, N)
fold_id = sample(fold_id, replace = FALSE)

#Compute cross validation error for Model1 
err_save = rep(0, K)
for (i in 1:K) {
  train_set = which(fold_id != i)
  y_testCV = greenbuildingsNoNA$Rent[-train_set]
  lm_RentPriceCV = lm(Rent ~ cluster + size + empl_gr  + leasing_rate 
                    + stories + stories*size + cd_total_07*Electricity_Costs
                    + age + renovated + class_a + class_b + green_rating + net 
                    + amenities + cd_total_07 + hd_total07 + Precipitation 
                    + Gas_Costs + Electricity_Costs + Electricity_Costs*hd_total07 
                    , data=greenbuildingsNoNA[train_set,]) 
  
  yhat_RentPrice_CV = predict(lm_RentPriceCV, newdata = greenbuildingsNoNA[-train_set,])
  
  err_save[i] = mean((y_testCV - yhat_RentPrice_CV)^2)
}
RMSE = sqrt(mean(err_save))

#Compute Cross Validation Method for Forward Selection Model  
err_saveForward = rep(0, K)
for (i in 1:K) {
  train_set = which(fold_id != i)
  y_testCV = greenbuildingsNoNA$Rent[-train_set]
 
  lm_Rent_ForwardCV = update(lm_forward, data = greenbuildingsNoNA[train_set,])
  
  yhat_lm_RentForward_CV = predict(lm_Rent_ForwardCV, newdata = greenbuildingsNoNA[-train_set,])
  
  err_saveForward[i] = mean((y_testCV - yhat_lm_RentForward_CV)^2) 
}
RMSE2 = sqrt(mean(err_saveForward))  

#compute cross validation error for Lasso Model 
err_saveLasso = rep(0, K)
for (i in 1:K) {
  train_set = which(fold_id != i)
  y_testCV = greenbuildingsNoNA$Rent[-train_set] 
  x = model.matrix(Rent ~ cluster + size + empl_gr  + leasing_rate + stories
                   + age + renovated + class_a + class_b + green_rating + net 
                   + amenities + cd_total_07 + hd_total07 + Precipitation 
                   + Gas_Costs + Electricity_Costs, data=greenbuildingsNoNA)[,-1]
  x = scale(x, center=TRUE, scale=TRUE) 
  y = greenbuildingsNoNA$Rent
  grid=10^seq(10,-2, length =100)
  lasso.mod=glmnet(x[train_set,],y[train_set],alpha=1, lambda =grid)
  cv.out=cv.glmnet(x[train_set,],y[train_set],alpha=1)
  bestlam =cv.out$lambda.min
  
  yhat_lm_Rent_Lasso = predict(lasso.mod, s=bestlam, newx=x[-train_set,])
  
  err_saveLasso[i] = mean((y_testCV - yhat_lm_Rent_Lasso)^2)
}
RMSE3 = sqrt(mean(err_saveLasso))

err_saveTreeBag = rep(0, K)
for (i in 1:K) {
  train_set = which(fold_id != i)
  greenbuildings_train = greenbuildingsNoNA[train_set,]
  #train_set = scale(train_set, center=TRUE, scale=TRUE) 
  y_testCV = greenbuildingsNoNA$Rent[-train_set] 
  y = greenbuildingsNoNA$Rent
  greenbuildings_test = greenbuildingsNoNA[-train_set,]
  
  set.seed(1)
  greenbuildBagging = randomForest(Rent ~ cluster + size + empl_gr  + leasing_rate + stories
                                   + age + renovated + class_a + class_b + green_rating + net 
                                   + amenities + cd_total_07 + hd_total07 + Precipitation 
                                   + Gas_Costs + Electricity_Costs, data = greenbuildings_train,
                                   mtry=17, importance=TRUE)
  
  yhat_greenbuildBagging = predict(greenbuildBagging, newdata = greenbuildings_test)
  
  err_saveTreeBag[i] = mean((y_testCV - yhat_greenbuildBagging)^2)
}
RMSE4 = sqrt(mean(err_saveTreeBag))


err_saveTreeForest = rep(0, K)
for (i in 1:K) {
  train_set = which(fold_id != i)
  greenbuildings_train = greenbuildingsNoNA[train_set,]
  #train_set = scale(train_set, center=TRUE, scale=TRUE) 
  y_testCV = greenbuildingsNoNA$Rent[-train_set] 
  y = greenbuildingsNoNA$Rent
  greenbuildings_test = greenbuildingsNoNA[-train_set,]
  
  set.seed(1)
  greenbuildRandomForest = randomForest(Rent ~ cluster + size + empl_gr  + leasing_rate + stories
                                        + age + renovated + class_a + class_b + green_rating + net 
                                        + amenities + cd_total_07 + hd_total07 + Precipitation 
                                        + Gas_Costs + Electricity_Costs, data = greenbuildings_train,
                                        mtry=4, importance=TRUE)
  
  yhat_greenbuildRandomForest = predict(greenbuildRandomForest, newdata = greenbuildings_test)
  
  err_saveTreeForest[i] = mean((y_testCV - yhat_greenbuildRandomForest)^2)
}
RMSE5 = sqrt(mean(err_saveTreeForest))

AvgRMSEModels = c("LOOCV RMSE Rent Hand-Built Model"=sqrt(mean(err_save)),
                                       "LOOCV RMSE Rent Forward Selection Model" = sqrt(mean(err_saveForward)), 
                                       "LOOCV RMSE Model Lasso Model" = sqrt(mean(err_saveLasso)),
                                       "LOOCV RMSE Model Bagging Model" = sqrt(mean(err_saveTreeBag)),
                                       "LOOCV RMSE Model RandomForest Model" = sqrt(mean(err_saveTreeForest)))

kable(AvgRMSEModels, col.names = c("LOOCV RMSE"), caption = "**Table 1.3 LOOCV RMSE per Model**",  format_caption = c("italic", "underline")) %>%
  kable_styling(bootstrap_options = "striped", full_width = F)
```

**Random Forest as the best predictive model possible** 

The tree decision models perform almost twice better than the remaining three models, based on the RMSE. We decided to use Random Forest as a predictive model for rent since its does not differ as much from the RMSE of bagging, and the procedure it uses usually results in an improvement over bagging as mentioned before. 

Below is a list of the variable importance for each variable used in Random Forest. The first column shows the increase in Mean Squared Error if that particular variable is omitted from the model. The MSE is calculated based on the out of bag samples. The second column shows the increase in node purity that results from splits over that variable. The increase in node purity is averaged over all trees. As shown below, the most important variable based on MSE is "age" because if omitted, the MSE increases the most. However, Electricity costs and Size increase the most node purity. 



```{r setup 1.11, warning=FALSE, echo=FALSE}

greenbuildings <- read.csv("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/greenbuildings.csv")

greenbuildingsNoNA <- na.omit(greenbuildings)

set.seed(1)
greenbuildRandomForest2 = randomForest(Rent ~ cluster + size + empl_gr  + leasing_rate + stories
                                      + age + renovated + class_a + class_b + green_rating + net 
                                      + amenities + cd_total_07 + hd_total07 + Precipitation 
                                      + Gas_Costs + Electricity_Costs, data = greenbuildingsNoNA,
                                      mtry=4, importance=TRUE)

VariableImp = as.data.table(importance(greenbuildRandomForest2), keep.rownames = TRUE)
kable(VariableImp, caption = "**Table 1.4 Variable Importance in Random Forest Model**", col.names = c("Predictor", "% Increase in MSE", "Increase in Node Purity"),  format_caption = c("italic", "underline")) %>%
  kable_styling(bootstrap_options = "striped", full_width = F)

```

### The Effect of Green Rating on Rent

Figure 1.5 illustrates the partial dependence plot of the marginal effect that the green rating has on the outcome of Random Forest. 

```{r setup 1.12, warning=FALSE, echo=FALSE}

greenbuildings <- read.csv("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/greenbuildings.csv")

greenbuildingsNoNA <- na.omit(greenbuildings)

partialPlot(greenbuildRandomForest, greenbuildingsNoNA, green_rating, main = "Figure 1.5: Partial Dependence on Green Rating" )

```

It is difficlut to esitmate the partial effect of green_rating on rent in a random forest model since tree based models are nonlinear in coefficients. So, by random forest, we can only find approximation to the average effect of green_rating. 

To find the average change in rental income per square foot considering green ratings and holding all other variables fixed, we calculated the difference between predicted values given a green rating and a non-green rating keeping all other variables the same. (Since green ratings is a dummy variable, to find the effect of the variable, we subtracted all predicted values given green_rating=0 from predicted values given green_rating=1, holding all other variables constant.) Afterwards, we took the average of the differences of predicted values to find the average effect of green ratings on rent price. Therefore, the average effect of green ratings on the rent price is approximately "0.3". This means that if the building is green then the rent will be on average 0.3 dollars per square foot per calendar year more expensive than if the building is non-green, holding all else fixed. 

```{r setup 1.13, warning=FALSE, echo=FALSE}

greenbuildings <- read.csv("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/greenbuildings.csv")

greenbuildingsNoNA <- na.omit(greenbuildings)

set.seed(1)
greenbuildRandomForest3 = randomForest(Rent ~ cluster + size + empl_gr  + leasing_rate + stories
                                      + age + renovated + class_a + class_b + green_rating + net 
                                      + amenities + cd_total_07 + hd_total07 + Precipitation 
                                      + Gas_Costs + Electricity_Costs, data = greenbuildingsNoNA,
                                      mtry=4, importance=TRUE)

GreenRating0 = replace(greenbuildingsNoNA, "green_rating", 0)
GreenRating1 = replace(greenbuildingsNoNA, "green_rating", 1)

PredictGrRating0 = predict(greenbuildRandomForest3, GreenRating0)
PredictGrRating1 = predict(greenbuildRandomForest3, GreenRating1)
DifferencePredict0m1 = data.table::data.table("DifferencePredict0m1" = PredictGrRating1 - PredictGrRating0)

DiffMean = mean(DifferencePredict0m1$DifferencePredict0m1)

kable(DiffMean, col.names = c("Average Green Rating Effect on Rent"), caption = "**Table 1.4 Average Effect on the Green Rating Variable on Rent**", format_caption = c("italic", "underline")) %>%
  kable_styling(bootstrap_options = "striped", full_width = F)
```

### Conclusions: 

1)	The best predictive models possible for price are the tree-based models, Random Forest and Bagging; 
2)	The average change in rental income per square foot associated with green certification, holding other features of the building constant, is 0.3 dollars per square foot. 


# Problem 2. What causes what?

### Question 1: Why can’t I just get data from a few different cities and run the regression of “Crime” on “Police” to understand how more cops in the streets affect crime? (“Crime” refers to some measure of crime rate and “Police” measures the number of cops in a city.)

It is not a good practice to just collect data from different cities and run a regression of “Crime” on “Police” because of the correlation vs. causality problem. Crime does not depend only on the amount of police force hired but also on other different factors, which should be accounted for. For instance, different locations have different demographics and crime challenges. Hence, these social and economic factors need to be accounted for before running a regression on “Crime”. However, if running a multiple regression of “Crime” on “Police” and other factors, the problem of causality is still not resolved. For example, bigger cities have in general more crime, therefore, they hire more police officers, so there is a positive correlation which does not determine causality between the two variables.

### Question 2: How were the researchers from UPenn able to isolate this effect? Briefly describe their approach and discuss their result in the “Table 2” below, from the researchers' paper.

To isolate the effect of police on crime, the researchers employed natural experiment. They observed an event where the high amount of police officers on the streets was not caused by the crime levels in the city. The researchers’ strategy was to use the terrorism alert system, specifically in Washington, D.C. as a control variable. In D.C. the terrorist alert system makes use of colors for the level of terrorist alert, so when the alert goes to orange, more police officers are around the city to protect against potential terror attacks. In this natural experiment, the researchers assume that street crime and terrorism threat are unrelated, hence, the increase in D.C. police force is “exogenous” to the conventional crime rate. 

The researchers observed that the number of crimes committed when the color alert was “Orange” was lower than when the terrorism alert level was “Yellow”. The orange terrorism alert requires more police officers than the yellow alert. In addition, the researchers found that the decrease in crime was sharpest in the area where there were the most police officers assigned. Therefore, they were able to answer the question of the effect of police officers on crime by observing a natural experiment and assigning a variable that controls for the amount of police officers assigned in Washington, D.C. for a reason unrelated to crime. 

As Table 2 suggests, by running a basic regression, the researches have concluded that on high-alert days, total crime decreases by an average of 7 crimes per day. Controlling for the Metro passengers, the regression gives a result of an average decrease of 6 crimes per day. 

### Question 3: Why did they have to control for Metro ridership? What was that trying to capture?

The reason why the researchers are controlling for the Metro system ridership is to address the issue of an alternative hypothesis that tourism is reduced during high terrorist alert days, consequently, that can be the cause of the lower crime levels, not the increase police force. However, when the researchers have examined the daily data on the number of passengers using the Metro system during high alert terror days, they have found that the flock of riders has not been significantly diminished during these days. 

As seen from Table 2, the researchers found that changes in Metro riders is somehow correlated with the crime levels, but the correlation is very small: a 10% increase in the amount of passengers increases crime by only around 1.7 per day on average. Consequently, changes in the number of tourists cannot fully explain the change in crime, so there is no significant causal effect of the change in tourists on the crime levels in D.C. during high alert days when police force increases. 

### Question 4: Below I am showing you "Table 4" from the researchers' paper. Just focus on the first column of the table. Can you describe the model being estimated here? What is the conclusion?

The researchers have split Washington, D.C. into different districts because each district of a city is likely to have varying crime statistics, population income, educational level, and other socio-demographic factors that differ in each district. From table 4, it can be concluded that District 1 is likely to have the most increased police attention because it has the highest threat of a terrorist attack. Given that District 1 is comprised of the White House, Congress and other prominent government agencies, the researchers have run a regression on District 1 as a separate variable and all other districts because it is expected that most of the police force will be concentrated around District 1. 

Table 4 suggests that a fixed effect regression has been run with district fixed effects to control for differences in the districts of Washington, D.C. Looking at the first column of table 4, during days of high alert, crime in District 1 is expected to decrease by approximately 2.62, while in other districts by only 0.571. However, the coefficient of other districts is not statistically significant. As it can be seen, controlling for districts fixed effects also decreases the coefficient Metro ridership and makes it less statistically significant than the regression from Table 2. Comparing the results from Questions 2 and 3, where researchers have been concentrated on the whole city, it can be concluded that most of the crime decline is concentrated in District 1. 

The researchers suggest that the low decrease in crime in other districts can be due to the fact that the city police department may be diverting resources from other districts to District 1, hence, there may be actually a decreased police presence in some districts during high alert days.

In conclusion, the regressions run in both tables: Table 1 and Table 2 show evidence of decrease in crime when police presence is increased, establishing a causal relationship between police and crime by running a natural experiment where increased police presence is due to other factors than street crime. 


# Problem 3: Clustering and PCA
## Overview

In this exercise we are interested in applying and comparing unsupervised learning methods like K-means clustering and Principal Component Analysis to see whether they can successfully distinguish the red wines from the white wines, as well as sorting the higher from the lower quality of wines based on the dataset with 11 chemical properties of 6500 different bottles of vinho verde wine from northern Portugal. 

## Application of unsupervised learning methods to distinguish red wines from white wines

As a first step, the variables are explored to learn by which chemical properties the white and red wines differentiate most.

```{r Figure 1. boxplot_colors, echo = FALSE, warning=FALSE}
wine <- read.csv("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/wine.csv")
p1 <- qplot(x = color, y = fixed.acidity, data = wine, geom = "boxplot")
p2 <- qplot(x = color, y = volatile.acidity, data = wine, geom = "boxplot")
p3 <- qplot(x = color, y = citric.acid, data = wine, geom = "boxplot")
p4 <- qplot(x = color, y = residual.sugar, data = wine, geom = "boxplot")
p5 <- qplot(x = color, y = chlorides, data = wine, geom = "boxplot")
p6 <- qplot(x = color, y = free.sulfur.dioxide, data = wine, geom = "boxplot")
p7 <- qplot(x = color, y = total.sulfur.dioxide, data = wine, geom = "boxplot")
p8 <- qplot(x = color, y = density, data = wine, geom = "boxplot")
p9 <- qplot(x = color, y = pH, data = wine, geom = "boxplot")
p10 <- qplot(x = color, y = sulphates, data = wine, geom = "boxplot")
p11 <- qplot(x = color, y = alcohol, data = wine, geom = "boxplot")
p12 <- qplot(x = color, y = quality, data = wine, geom = "boxplot")
grid.arrange(p1, p2, p3, p4, p5, p6, p7, p8, p9, p10, p11, p12, nrow = 3, top = "Figure 3.1 Boxplots of chemical properties of white and red wines")

```

From these boxplots, it can be concluded that red and white wines differentiate mostly by 
chemical properties like total sulphur dioxide, residual sugar, volatile acidity and  density. These features will be used later to analyze the performance of the selected methods.

#### K-means clustering to distinguish colors of wine

K-means clustering was chosen as a first approach to differentiate the two types of wines by color. Accordingly, we set the number of clusters K in K-means as 2.Then we run K-means on 11 chemical properties(the first 11 columns of the dataset).   

The following two figures show visually that K-means clustering could successfully differentiate two colors of wines in relation to four chemical properties, by which red and white wines differentiate mostly. Additionally, in the first figure of boxplots we noticed that two colors of wines didn't differentiate much in the other chemical features. For this reason we chose to show visually the performance of K-means clustering on these four chemical features. For example, in the Figure 3.2. the top panel shows how k-means clustering partitioned the data into two clusters in the dimensions of fixed acidity and residual sugar. The bottom panel shows how actual wine colors differentiate in these dimensions. It can be inferred, that the two panels appear to be very similar visually. Figure 3.3. follows similar pattern.

```{r figure 3.2-3.3. K-means_colors, echo = FALSE, warning=FALSE}
wine <- read.csv("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/wine.csv")
# Center and scale the data
Xw = wine[,(1:11)]
Xw = scale(Xw, center=TRUE, scale=TRUE)
# Extract the centers and scales from the rescaled data (which are named attributes)
mu = attr(Xw,"scaled:center")
sigma = attr(Xw,"scaled:scale")
# Run k-means with 2 clusters and 25 starts
set.seed(1)
clust1w = kmeans(Xw, 2, nstart=25)

p1.1 <- qplot(fixed.acidity, residual.sugar , data=wine, color=factor(clust1w$cluster))
p1.2 <- qplot(fixed.acidity, residual.sugar , data=wine, color=factor(wine$color))
grid.arrange(p1.1, p1.2, nrow = 2, top = "**Figure 3.2. Results of K-means clustering for wine colors 
in the dimensions of fixed acidity and residual sugar**")

p1.3 <- qplot(total.sulfur.dioxide, volatile.acidity, data=wine, color=factor(clust1w$cluster))
p1.4 <- qplot(total.sulfur.dioxide, volatile.acidity, data=wine, color=factor(wine$color))
grid.arrange(p1.3, p1.4, nrow = 2, top = "**Figure 3.3 Results of K-means clustering for wine colors 
in the dimensions of total sulfur dioxide and volatile acidity**")

```


#### Prinicipal Components Analysis (PCA) to distinguish colors of wine

Next, Prinipal Components Analysis was performed in a similar attempt to distinguish the wines by two colors. The table below shows that the first five principal components explain cumulatively about 80% of the variation in the data. 


```{r , summary_pcw1, figure 3.4. PVE of pca, echo = FALSE, warning=FALSE}
wine <- read.csv("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/wine.csv")
pcw1 = prcomp(Xw, scale=TRUE)

sum_pcw1=data.frame(summary(pcw1)$importance)
sum_pcw1=sum_pcw1[ , 1:11]
kable(sum_pcw1, caption="**Table 3.1. Summary table of PCA to distinguish colors**", format_caption = c("italic", "underline"), booktabs=TRUE) %>%
  kable_styling(bootstrap_options = "striped", full_width = T)

plot(pcw1, main = "Figure 3.4. Proportions of variance explained by\n each principal component", xlab='Principal Components')
loadings = pcw1$rotation
```

Below we can examine the loadings of the first 5 principal components. These are the linear combinations of the original chemical properties, which preserve as much of the information of those variables as possible. For example, free sulfur dioxide and total sulfur dioxide are relatively strongly and positively correlated with the first principal component, whereas volatile acidity and sulphates are relatively strongly and negatively correlated with the first principal component.

```{r , first five PCs, echo = FALSE, warning=FALSE}
wine <- read.csv("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/wine.csv")
rot_pcw1=data.frame(round(pcw1$rotation[,1:5],2))
kable(rot_pcw1, caption="**Table 3.2. Loadings of the first five principal components**", format_caption = c("italic", "underline")) %>%
  kable_styling(bootstrap_options = "striped", full_width = F)
```
Next, answering the question of where the individual data points fall in the space defined by first two principal components can help us identify how PCA performed the clustering of wines by their color. 

```{r , figure 1.5. pca for wine colors , echo = FALSE, warning=FALSE}
wine <- read.csv("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/wine.csv")
scores = pcw1$x
qplot(scores[,1], scores[,2], color=wine$color, xlab='Component 1', ylab='Component 2', main = "Figure 3.5. Results of PCA for wine colors in the dimensions 
      of first two principal components")
```

As it can be seen from the plot above, PCA could also perform the clustering of two types of wine. Mainly, white wines tend to cluster to the positive direction in the dimension of first principal component, whereas red wines tend to cluster in the opposite direction.

Further, PCA results can be used to perform K-means clustering based on them. We performed the K-means clustering on the first five principal components to check whether we can enhance the partitioning of wine color done by K-means. Figure 3.6. illustrates that differentiation of wine colors was done very successfully by these augmented k-means clustering method.

```{r , k-means via pca colors, echo = FALSE, warning=FALSE}
wine <- read.csv("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/wine.csv")
clust1wpca = kmeans(scores[,1:5], 2, nstart=20)
p3.1 <- qplot(fixed.acidity, residual.sugar , data=wine, color=factor(clust1wpca$cluster))
p3.2 <- qplot(fixed.acidity, residual.sugar , data=wine, color=factor(wine$color))
grid.arrange(p3.1, p3.2, nrow = 2, top = "Figure 3.6 Results of K-means clustering via PCA for wine colors 
in the dimensions of total sulfur dioxide and volatile acidity.")  
```

Table 3.3. compares the results of two clustering of K=2 based on the total within-cluster sum of squares and the between-cluster sum of squares. Clustering via PCA has a relative improvement of about 25% in the total within-cluster sum of squares, and approximately the same between-cluster sum of squares.

```{r , table 3.3 SSE comparison, echo = FALSE, warning=FALSE}
wine <- read.csv("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/wine.csv")

totwithinbetweenss = rbind("Total within-cluster sum of squares of Basic clustering " = clust1w$tot.withinss, 
                 "Total within-cluster sum of squares of Clustering via PCA " = clust1wpca$tot.withinss, 
                 "Between-cluster sum of squares of Basic clustering " = clust1w$betweenss, 
                 "Between-cluster sum of squares of Clustering via PCA " = clust1wpca$betweenss)
kable(totwithinbetweenss, caption="**Table 3.3 : Evaluating in-sample fit of two clusterings**")%>%
  kable_styling(full_width = FALSE)
```

To sum up, it can be said that K-means and PCA  could succesfully differentiate the white and red wines using only the “unsupervised information” contained in the data on chemical properties of wines. However, by reducing the dimensions of the features of the dataset by PCA and then applying K-means clustering could distinguish colors of wine at even better level, which could be seen from the reduced within-cluster sum of squares.

## Application of unsupervised learning methods to distinguish quality levels of wines

Before applying the unsupervised learning methods to sort the higher levels of wines from the lower levels, we can look at the distribution of various wine quality levels in this dataset.
```{r , Figure 3.7. histogram of quality levels, echo = FALSE, warning=FALSE}
wine <- read.csv("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/wine.csv")
ggplot(wine)+
  geom_bar(aes(x = quality, fill = color, binwidth = 1))+
             labs(title = "Figure 3.7. Histogram of wine qualities")
```

The figure 3.7. shows that there are 7 different qualities of wines in the data. As we are specifically interested in distinguishing between higher types of wines from the lower types, we have divided the wines into three main levels of qualities. Wines that scored higher that 7 were defined as high quality wines, lower than 5 - as low quality and from 5 to 7 as medium quality wines. 
As a next step, variables were explored to know by which chemical properties wine quality levels differentiate most.

```{r , Figure 3.8. Boxplots of chemical prop qualities, echo = FALSE, warning=FALSE}
wine <- read.csv("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/wine.csv")
wine$qualityindicator <- ifelse(wine$quality <= 4, 'low', ifelse(wine$quality <= 7, 'medium', 'high'))
pq1 <- qplot(x = qualityindicator, y = fixed.acidity, data = wine, geom = "boxplot")
pq2 <- qplot(x = qualityindicator, y = volatile.acidity, data = wine, geom = "boxplot")
pq3 <- qplot(x = qualityindicator, y = citric.acid, data = wine, geom = "boxplot")
pq4 <- qplot(x = qualityindicator, y = residual.sugar, data = wine, geom = "boxplot")
pq5 <- qplot(x = qualityindicator, y = chlorides, data = wine, geom = "boxplot")
pq6 <- qplot(x = qualityindicator, y = free.sulfur.dioxide, data = wine, geom = "boxplot")
pq7 <- qplot(x = qualityindicator, y = total.sulfur.dioxide, data = wine, geom = "boxplot")
pq8 <- qplot(x = qualityindicator, y = density, data = wine, geom = "boxplot")
pq9 <- qplot(x = qualityindicator, y = pH, data = wine, geom = "boxplot")
pq10 <- qplot(x = qualityindicator, y = sulphates, data = wine, geom = "boxplot")
pq11 <- qplot(x = qualityindicator, y = alcohol, data = wine, geom = "boxplot")
grid.arrange(pq1, pq2, pq3, pq4, pq5, pq6, pq7, pq8, pq9, pq10, pq11, nrow = 3, top = "Figure 3.8. Boxplots of chemical properties of three wine quality levels")
```

From these boxplots, it can be concluded that higher and lower quality wines differentiate mostly by chemical properties like volatile acidity, alcohol, free sulfur dioxide and chlorides. These features will be used later to analyze the performance of the selected methods.

#### K-means clustering to distinguish between higher and lower quality levels of wines

K-means clustering was performed to test whether it can also  differentiate the types of wines by quality level. As we have three main levels of qualities, accordingly the number of clusters K in K-means is 3.  

The following set of figures show visually results of K-means clustering in differentiating the qualities of wines in relation to four chemical properties. For example, in the Figure 3.9. the top left plot shows how k-means clustering partitioned the data into three clusters in the dimensions of volatile acidity and alcohol. The middle left plot shows how actual three wine quality levels differentiate in these dimensions. It is difficult to see, how lower and higher quality wines differ, because of the huge amount of middle quality wines that overlap with them, so the bottom left plot shows only the high and low qualities of wines in these dimensions. Although, the distribution pattern of low and high qualities appear to be similar with red and green clusters in the top plot, it cannot be said that they follow exact pattern as was in the case of partitioning the colors of wine. The right panel shows this comparison in the dimensions of other two chemical features.

```{r , Figure 3.9 K-means for qualities, echo = FALSE, warning=FALSE}
wine <- read.csv("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/wine.csv")
wine$qualityindicator <- ifelse(wine$quality <= 4, 'low', ifelse(wine$quality <= 7, 'medium', 'high'))
set.seed(3)
c = kmeans(Xw, 3, nstart=25)
pq1.1 <- qplot(alcohol, volatile.acidity,  data=wine, color=factor(c$cluster), alpha=I(0.3))
pq1.2 <- ggplot(wine, aes(x=alcohol, y = volatile.acidity, color=qualityindicator, alpha = I(0.3)))+
  geom_point(data = subset(wine, qualityindicator %in% c("low","high", "medium")))
pq1.8 <- ggplot(wine, aes(x=alcohol, y = volatile.acidity, color=qualityindicator, alpha = I(0.3)))+
  geom_point(data = subset(wine, qualityindicator %in% c("low","high")))
pq1.5 <- ggplot(wine, aes(free.sulfur.dioxide, chlorides))+
  geom_point(aes(color=factor(c$cluster),alpha = I(0.3)))
pq1.6 <- ggplot(wine, aes(x=free.sulfur.dioxide, y = chlorides, color=qualityindicator, alpha = I(0.3)))+
  geom_point(data = subset(wine, qualityindicator %in% c("low","high","medium")))
pq1.10 <- ggplot(wine, aes(x=free.sulfur.dioxide, y = chlorides, color=qualityindicator, alpha = I(0.3)))+
  geom_point(data = subset(wine, qualityindicator %in% c("low","high")))
grid.arrange(pq1.1, pq1.5, pq1.2, pq1.6, pq1.8, pq1.10, nrow = 3, top = "Figure 3.9. Results of K-means clustering for wine qualities.")
```

#### Prinicipal Components Analysis (PCA) to distinguish between higher and lower quality levels of wines

Next, Prinipal Components Analysis was performed in a similar attempt to distinguish the quality levels of wines. From the previous PCA on wine colors it is known that the first five principal components explain cumulatively about 80% of the variation in the data. As before,by answering the question of where the individual data points fall in the space defined by first two principal components we can identify how PCA performed the clustering of wines by their quality levels.

```{r , figure 3.10. pca qualities, echo = FALSE, warning=FALSE}
wine <- read.csv("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/wine.csv")
wine$qualityindicator <- ifelse(wine$quality <= 4, 'low', ifelse(wine$quality <= 7, 'medium', 'high'))
pc_wine_qualities = prcomp(Xw, scale=TRUE)
scores2 = pc_wine_qualities$x
qplot(scores2[,1], scores2[,2], color=wine$qualityindicator, xlab='Component 1', ylab='Component 2', main = "Figure 3.10. Results of PCA for wine qualities in the dimensions 
      of first two principal components", alpha=I(0.1))
```

As it can be seen from the plot above, now PCA couldn't perform the clustering of higher qualities of wine from the lower types in a very distinctive way. Mainly, higher types of wines tend to cluster to the right spectrum of the dimension of first principal component, whereas lower quality wines tend to cluster in the left.

With the same procedure as before, PCA results were used to perform K-means clustering based on them. We performed the K-means clustering on the first five principal components to check whether partitioning of wine quality types done by K-means could be enhanced. Figure 3.11 illustrates that differentiation of wines by quality types was done at a better level by these augmented k-means clustering method. For example, the top left plot shows how k-means clustering partitioned the data into three clusters in the dimensions of volatile acidity and alcohol. The middle left plot shows how actual three wine quality levels differentiate in these dimensions. It is difficult to see, how lower and higher quality wines differ, because of the huge amount of middle quality wines that overlap with them, so the bottom left plot shows only the high and low qualities of wines in these dimensions. Although the distribution pattern of low and high qualities appears to be similar with blue and green clusters in the top plot, it cannot be said that they follow exact same pattern. The right panel shows this comparison in the dimensions of other two chemical features like chlorides and free sulfur dioxide.

```{r , figure 3.11. k-means via pca qualities, echo = FALSE, warning=FALSE}
wine <- read.csv("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/wine.csv")
wine$qualityindicator <- ifelse(wine$quality <= 4, 'low', ifelse(wine$quality <= 7, 'medium', 'high'))
set.seed(4)
pc = kmeans(scores2[,1:5], 3, nstart=20)
pq1.11 <- qplot(alcohol, volatile.acidity,  data=wine, color=factor(pc$cluster), alpha=I(0.3))
pq1.12 <- ggplot(wine, aes(x=alcohol, y = volatile.acidity, color=qualityindicator, alpha = I(0.3)))+
  geom_point(data = subset(wine, qualityindicator %in% c("low","high", "medium")))
pq1.18 <- ggplot(wine, aes(x=alcohol, y = volatile.acidity, color=qualityindicator, alpha = I(0.3)))+
  geom_point(data = subset(wine, qualityindicator %in% c("low","high")))
pq1.15 <- ggplot(wine, aes(free.sulfur.dioxide, chlorides))+
  geom_point(aes(color=factor(pc$cluster),alpha = I(0.3)))
pq1.16 <- ggplot(wine, aes(x=free.sulfur.dioxide, y = chlorides, color=qualityindicator, alpha = I(0.3)))+
  geom_point(data = subset(wine, qualityindicator %in% c("low","high","medium")))
pq1.110 <- ggplot(wine, aes(x=free.sulfur.dioxide, y = chlorides, color=qualityindicator, alpha = I(0.3)))+
  geom_point(data = subset(wine, qualityindicator %in% c("low","high")))
grid.arrange(pq1.11, pq1.15, pq1.12, pq1.16, pq1.18, pq1.110, nrow = 3, top = "Figure 3.11. Results of K-means clustering via PCA for wine qualities.")
```

Table 3.4. compares the results of two clustering with K=3 based on the total within-cluster sum of squares and the between-cluster sum of squares. Clustering via PCA has a relative improvement of about 31% in the total within-cluster sum of squares, and approximately the same between-cluster sum of squares.

```{r , table 3.4 SSE comparison qualities, echo = FALSE, warning=FALSE}
wine <- read.csv("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/wine.csv")

totwithinbetweenss = rbind("Total within-cluster sum of squares of Basic clustering " = c$tot.withinss, 
                 "Total within-cluster sum of squares of Clustering via PCA " = pc$tot.withinss, 
                 "Between-cluster sum of squares of Basic clustering " = c$betweenss, 
                 "Between-cluster sum of squares of Clustering via PCA " = pc$betweenss)
kable(totwithinbetweenss, caption="**Table 3.4 : Evaluating in-sample fit of two clusterings**")%>%
  kable_styling(full_width = FALSE)
```

To sum up, it can be said that neither K-means clustering nor PCA could very succesfully differentiate the higher from the lower quality wines using only the "unsupervised information" contained in the data on chemical properties of wines. However, by reducing the dimensions of the features of the dataset by PCA and then applying K-means clustering could distinguish quality levels of wine at better level, which could be seen from the reduced within-cluster sum of squares.

## Conclusion

In conclusion, we could say that K-means clustering applied together with PCA as a dimensionality reduction technique for the chemical properties of wines works better to differentiate the red and white colors of wine, compared to the same technique to distinguish higher from the lower quality wines.


# Problem 4: Market Segmentation
## Overview
Based on data of the its twitter account followers, the brand “NutrientH20” tries to find out possible approaches to segment those followers. The key for our market segmentation purpose is to find some identifiers to identify different clusters of consumers so that NutrientH20 can target its consumers more accurately in marketing or advertising. In analysis, we used three approaches to get there: K-means, Principal Component Analysis (PCA) with Hierarchical Clustering, and Hierarchical Clustering with K-means. The first two aimed at doing general market segmentation, and the third aimed at giving the previous clusters more accurate labels. 

## The Data
The data contains 7828 twitter followers (consumers) and 36 categories of their twitter posts (tweets). For each follower, the data gives the amount of tweets in each category, during seven days in June 2014. First, we deleted the observations with either nonzero “spam” tweets or nonzero “adult” tweets since most of those two kinds of tweets are likely posted by robots that are meaningless followers for marketing analysis. Then, we deleted “spam “and “adult”. At last, we transformed the number of tweets in each category into the frequency of tweets belonging to each category to better represent the preferences of consumers. After doing all these, we got a dataset with 7309 consumers (rows) and 34 categories.

## K-means
### The Proper K
K-means is a method to partition observations or features with a high dimension P into lower K dimensions (P>=K). For our purpose, it is to partition 7309 consumers into K clusters. We used CH index to choose a proper value of K.

```{r setup 4.1, results='hide', warning=FALSE, echo=FALSE}
socialmarketingraw=read.csv('https://raw.githubusercontent.com/jgscott/ECO395M/master/data/social_marketing.csv')
socialmarketing=filter(socialmarketingraw, socialmarketingraw$spam == 0 & socialmarketingraw$adult == 0)
X = socialmarketing[ , c(-1, -36, -37)]
X=X/rowSums(X)
Xs=scale(X, center = TRUE, scale = TRUE)

Ns = nrow(Xs)
k_grid = seq(2, 20, by=1)
CH_grids = foreach(k = k_grid, .combine='c') %do% {
  cluster_ks = kmeans(Xs, k, nstart=25)
  W = cluster_ks$tot.withinss
  B = cluster_ks$betweenss
  CH = (B/W)*((Ns-k)/(k-1))
}
#gap=clusGap(Xs, FUN=kmeans, nstart=25, K.max = 20, B=50)
plot(k_grid, CH_grids, main="Figure 4.1 CH Index")
#plot(gap)
```

The Figure 4.1 is the elbow plot of CH index. It is not so straightforward to choose a vivid elbow from the graph. Generally, values of K ranged from 10 to 15 seemed to be good candidates. To make the clustering as parsimonious as possible, we chose K = 10.

### The Result of K-means
K-means gave us 10 stable clusters of consumers. Then, for each cluster, we calculated the sum of frequencies of each category over all consumers in that cluster. After ordering the categories by sums of frequencies, the first five categories with higher sums of frequencies were picked out to represent that cluster. Also, the number of consumers in each cluster was counted. The ultimate result is shown in Table 4.1. 

```{r setup 4.2,  warning=FALSE, echo=FALSE}
socialmarketingraw=read.csv('https://raw.githubusercontent.com/jgscott/ECO395M/master/data/social_marketing.csv')
socialmarketing=filter(socialmarketingraw, socialmarketingraw$spam == 0 & socialmarketingraw$adult == 0)
X = socialmarketing[ , c(-1, -36, -37)]
X=X/rowSums(X)
Xs=scale(X, center = TRUE, scale = TRUE)

set.seed(1)
clust_k= kmeans(Xs, 10, nstart=25)
#fviz_cluster(clust_k, data = Xs)

result=data.frame(clust_k$cluster)

resultCombinedData=cbind(X, result$clust_k.cluster)

table_kmeans=data.frame("Cluster"=character(7))
for (i in 1:10) {
  Cluster = filter(resultCombinedData, result$clust_k.cluster == i)
  ClusterMeans = as.data.table(colMeans(Cluster), keep.rownames=TRUE)
  colnames(ClusterMeans) <- c("Categories", "Cluster Means")
  ClusterMeans = ClusterMeans[order(-`Cluster Means`)]
  ClusterMeans = ClusterMeans[1:6,1]
  c=data.frame(nrow(Cluster))
  ClusterMeans=rbind(ClusterMeans,c, use.names=FALSE)
  table_kmeans=cbind(table_kmeans, ClusterMeans)
}
table_kmeans=table_kmeans[-1, -1]
table_kmeans=transpose(table_kmeans)
table_kmeans=table_kmeans[order(table_kmeans[, ncol(table_kmeans)], decreasing = TRUE),]
rownames(table_kmeans)=c("Cluster 1", "Cluster 2", "Cluster 3", "Cluster 4", "Cluster 5", "Cluster 6",
                         "Cluster 7", "Cluster 8", "Cluster 9", "Cluster 10")
colnames(table_kmeans)=c("Category 1", "Category 2", "Category 3", "Category 4", "Category 5",
                         "Consumer No.")
kable(table_kmeans, caption ="**Table 4.1: Ten Clusters of K-means**" ,format_caption = c("bold", "underline")) %>%
  kable_styling(bootstrap_options = "striped", full_width = F)
```

We briefed those clusters below. 

**Cluster 1** (967 consumers): sports_fandom, religion, food, chatter, parenting
(This cluster probably had more parents and adults with relatively greater ages.)

**Cluster 2** (734 consumers): cooking, photo_sharing, fashion, chatter, beauty
(This cluster probably contained more consumres interested in leisure and lifestyle.)

**Cluster 3** (603 consumers): chatter, tv_film, college_uni, current_events, music
(This cluster had more college student interested in movie, TV shows, and music.)

**Cluster 4** (582 consumers): news, politics, chatter, automotive, sports_fandom
(This cluster focused more on news and politics.)

**Cluster 5** (553 consumers):  politics, travel, chatter, computers, news
(This cluster had consumers interested in topics like politics, travel, and computers. They also cared about news.)

**Cluster 6** (507 consumers): college_uni, online_gaming, chatter, photo_sharing, sports_playing
(This cluster more likely contained many college students interested in online gaming and online socializing.)

**Cluster 7** (346 consumers): art, tv_film, chatter, current_events, travel
(This cluster had consumers interested in arts, movies, TV shows, and probably viral events.)

**Cluster 8** (275 consumers): dating, chatter, photo_sharing, fashion, school
(This cluster also had many students, but probably relatively younger.)

**Cluster 9** (1564 consumers): chatter, photo_sharing, shopping, current_events, travel
(This cluster had consumers doing typical twitter related activities.)

**Cluster 10** (1178 consumers): health_nutrition, personal_fitness, chatter, cooking, outdoors
(This cluster had consumers interested in health, workout, and outdoor activities.) 

In addition, chatter was a common preference over most clusters.

## 4.4 The Principal Component Analysis (PCA) + Hierarchical Clustering
### Choose a Scaled Dataset and the Number of Principal Components
Next, we did segmentation by PCA. Since PCA is sensitive to the scales of data to some degree, before choosing a proper number of principal components, the first thing to do is to compare the effects of differently scaled data on PCA. We used three differently scaled datasets to do that. The first dataset was the one transformed into frequencies. The second one was gotten by doing zero-mean standardization on the frequency dataset. The third one was gotten from doing zero-mean standardization on the original dataset, the one before doing frequency transformation. PVE, the proportion of variance of each principal component, was calculated after doing PCA on each dataset. The Figure 4.1 shows how PVE of each dataset varies with each principal component, where the blue line corresponds to the first dataset, the red line corresponds to the second dataset, and the green one corresponds to the third dataset. Around the elbow part where the number of principal components is about eight to twelve, the blue line has the lowest position, which means the first several principal components of blue one summarize more variance in data. Hence, we chose the frequency dataset to do PCA.

```{r setup 4.3,  warning=FALSE, echo=FALSE}
socialmarketingraw=read.csv('https://raw.githubusercontent.com/jgscott/ECO395M/master/data/social_marketing.csv')
socialmarketing=filter(socialmarketingraw, socialmarketingraw$spam == 0 & socialmarketingraw$adult == 0)
X = socialmarketing[ , c(-1, -36, -37)]
X=X/rowSums(X)
Xs=scale(X, center = TRUE, scale = TRUE)

pca1=prcomp(X)
pve1=100*pca1$sdev^2/sum(pca1$sdev^2)

pca2=prcomp(X, scale=TRUE)
pve2=100*pca2$sdev^2/sum(pca2$sdev^2)

X2 = socialmarketing[ , c(-1, -36, -37)]
pca3=prcomp(X2, scale=TRUE)
pve3=100*pca3$sdev^2/sum(pca3$sdev^2)

PC=c(1:34)
pvecombine=cbind.data.frame(PC, pve1, pve2, pve3)
ggplot(data=pvecombine)+
  geom_line(aes(x=PC,y=pve1), col="blue")+geom_point(aes(x=PC, y=pve1), col="blue", shape=1, size=2)+
  geom_line(aes(x=PC,y=pve2), col="red")+geom_point(aes(x=PC, y=pve2), col="red", shape=2, size=2)+
  geom_line(aes(x=PC,y=pve3), col="green3")+geom_point(aes(x=PC, y=pve3), col="green3", shape=3, size=2)+
  labs(y="PVE", x="Principal Component")+
  theme_bw()+
  labs(title = "Figure 4.2: PVE of PCA Based on Datasets Scaled Differently")+
  theme(plot.title = element_text(hjust = 0.5))
```

Also based on the location of elbow in Figure 4.2, the first ten principal components were chosen as a proper summary of data. Table 4.2 also justifies our choosing: the first ten principal components include around 73.4 percent variation in data, which means the rest twenty-four principal components only represent 26.6 percent variation in data. So, we believe that the first ten are representative enough for the data.

```{r setup 4.4,  warning=FALSE, echo=FALSE}
socialmarketingraw=read.csv('https://raw.githubusercontent.com/jgscott/ECO395M/master/data/social_marketing.csv')
socialmarketing=filter(socialmarketingraw, socialmarketingraw$spam == 0 & socialmarketingraw$adult == 0)
X = socialmarketing[ , c(-1, -36, -37)]
X=X/rowSums(X)
Xs=scale(X, center = TRUE, scale = TRUE)

pca1=prcomp(X)
pve1=100*pca1$sdev^2/sum(pca1$sdev^2)

sum_pca1=data.frame(summary(pca1)$importance)
sum_pca1=sum_pca1[ , 1:10]
kable(sum_pca1, caption ="**Table 4.2: Summary of Variations in Principal Components**", format_caption = c("italic", "underline")) %>%
  kable_styling(bootstrap_options = "striped", full_width = T)

```

### The Potential Problems of First Ten Principal Components
The first ten principal components can be used to differentiate consumers. In each principal component, the importance of categories was measured by loadings. Those categories assigned with high positive loadings were supposed to be closer to each other than other categories so that they were more likely to be in the same cluster.The result was presented in Table 4.3. 

```{r setup 4.5,  warning=FALSE, echo=FALSE}
socialmarketingraw=read.csv('https://raw.githubusercontent.com/jgscott/ECO395M/master/data/social_marketing.csv')
socialmarketing=filter(socialmarketingraw, socialmarketingraw$spam == 0 & socialmarketingraw$adult == 0)
X = socialmarketing[ , c(-1, -36, -37)]
X=X/rowSums(X)
Xs=scale(X, center = TRUE, scale = TRUE)

pca1=prcomp(X)
pca1_loading=pca1$rotation[ , c(1:10)]

pc=data.frame(rep(1:34))
for (i in c(1:10)){
  sort=data.frame(sort(pca1_loading[,i], decreasing = TRUE))
  rownames=data.frame(row.names(sort))
  c=cbind(rownames, sort[,1])
  pc=cbind(pc,c)
}
pc=pc[,-1]
colnames(pc)=c("Categories", "PC1", "Categories", "PC2", "Categories", "PC3", "Categories", "PC4", "Categories", "PC5",
               "Categories", "PC6", "Categories", "PC7", "Categories", "PC8", "Categories", "PC9", "Categories", "PC10")

tablePCA=pc[1:5,-c(seq(2, 20, by=2))]

colnames(tablePCA)=c("PC 1", "PC 2", "PC 3", "PC 4", "PC 5", "PC 6",
                     "PC 7", "PC 8", "PC 9", "PC 10")
rownames(tablePCA)=c("Category 1", "Category 2", "Category 3", "Category 4", "Category 5")
kable(tablePCA,  caption ="**Table 4.3: The Result of First 10 Principal Components**", format_caption = c("italic", "underline")) %>%
  kable_styling(bootstrap_options = "striped", full_width = T)

```

But there are two problems in the above result: it includes only 21 categories out of 34, and we do not know yet how many consumers have the bigest interest in each cluster and who they are, which makes us a few miles away from our destination.

### Use Hierarchical Clustering to Make a Better Clustering
To solve the problems, we used the scores (indices to indicate the positions of each observation in the projected space) of PCA to construct a distance matrix, which measures the distance between each pair of consumers in the data. By Hierarchical Clustering and cutting the resulting dendrogram, we got 10 clusters of consumers, in accordance with the value of K in K-means. Afterwards, we did the same thing as in K-means after getting 10 clusters. The result is shown in Table 4.4 (To be more distinguishable, we named the clusters as “Pcluster” here). It is the exactly same as in K-means, which strongly implies the robustness of partitioning the market into 10 clusters.

```{r setup 4.6,  warning=FALSE, echo=FALSE}
socialmarketingraw=read.csv('https://raw.githubusercontent.com/jgscott/ECO395M/master/data/social_marketing.csv')
socialmarketing=filter(socialmarketingraw, socialmarketingraw$spam == 0 & socialmarketingraw$adult == 0)
X = socialmarketing[ , c(-1, -36, -37)]
X=X/rowSums(X)
Xs=scale(X, center = TRUE, scale = TRUE)

pca1=prcomp(X)
pca1_scores=pca1$x[ , c(1:10)]

D0=dist(pca1_scores)
hclust10 = hclust(D0, method='complete')
cluster_pca1_10=cutree(hclust10, 10)

resultp=data.frame(cluster_pca1_10)

resultpCombinedData=cbind(X, result$clust_k.cluster)

table_pca=data.frame("Cluster"=character(7))
for (i in 1:10) {
  Cluster = filter(resultpCombinedData, result$clust_k.cluster == i)
  ClusterMeans = as.data.table(colMeans(Cluster), keep.rownames=TRUE)
  colnames(ClusterMeans) <- c("Categories", "Cluster Means")
  ClusterMeans = ClusterMeans[order(-`Cluster Means`)]
  ClusterMeans = ClusterMeans[1:6,1]
  c=data.frame(nrow(Cluster))
  ClusterMeans=rbind(ClusterMeans,c, use.names=FALSE)
  table_pca=cbind(table_pca, ClusterMeans)
}

table_pca=table_pca[-1, -1]
table_pca=transpose(table_pca)
table_pca=table_pca[order(table_pca[, ncol(table_pca)], decreasing = TRUE),]
rownames(table_pca)=c("PCluster 1", "PCluster 2", "PCluster 3", "PCluster 4", "PCluster 5", "PCluster 6",
                         "PCluster 7", "PCluster 8", "PCluster 9", "PCluster 10")
colnames(table_pca)=c("Category 1", "Category 2", "Category 3", "Category 4", "Category 5",
                         "Consumer No.")
kable(table_pca,  caption ="**Table 4.4: An Improved Result of Hierarchical Clustering**", format_caption = c("italic", "underline")) %>%
  kable_styling(bootstrap_options = "striped", full_width = F)
```


## Can We Have More Accurate, Unique or Complete Labels?
The first problem of the result in Table 4.3 also exists in Table 4.2 and Table 4.4, though in a much less evident way: 27 categories out of 34 categories are used to differentiate consumers into 10 clusters. Probably it is because the rest 7 categories are not discernable or important enough based on the data, but it could be the possibility that our method was not refined enough. Of course, we could pick out more categories, say, first six or seven categories with high sums of frequencies, from each cluster, to include categories as many as possible. However, usually the categories below the third one only have relatively much smaller sums of frequencies than those of the first three categories. So, including more categories would be likely to put more infrequent or accident tweets into the description of distinct preferences. Moreover, some clusters may be not so distinguishable from other clusters. For example, Cluster 4 and 5, as well as Cluster 3 and 6, are more likely to consist of the same group of people. The borders between those clusters are not so clear. Therefore, in this part, for each of 10 clusters, we tried to find out the unique category or the combination of several unique categories dominating all other categories by the sum of frequencies. Meanwhile, all those unique category or combinations of unique categories made up a partition of 34 categories. By doing so, we were attaching labels to each cluster to make them more distinguishable.

The approach we applied was Hierarchical Clustering with K-means. The Hierarchical Clustering was to find out a hierarchical structure of categories based on the proximity matrix of correlations between each pair of categories. Figure 4.3 shows the dendrogram. 

```{r setup 4.7, results='hide', warning=FALSE, echo=FALSE}
socialmarketingraw=read.csv('https://raw.githubusercontent.com/jgscott/ECO395M/master/data/social_marketing.csv')
socialmarketing=filter(socialmarketingraw, socialmarketingraw$spam == 0 & socialmarketingraw$adult == 0)
X = socialmarketing[ , c(-1, -36, -37)]
X=X/rowSums(X)
Xs=scale(X, center = TRUE, scale = TRUE)

Xs_corr=cor(Xs)
d_Xscorr=dist(Xs_corr)

hc_corr1=hclust(d_Xscorr, method = "complete")
plot(hc_corr1, cex=0.8, xlab="", sub="", main="Figure 4.3: Dendrogram of Hierarchical Clustering of Categories")
```

Then, we horizontally cut the dendrogram into ten clusters of categories as a partition of 34 categories. Based on the partition, we added up the categories belonging to the same cluster in the frequency dataset, to get a new dataset with 7309 observations and 10 clusters. Then we used K-means to split consumers into 10 clusters of consumers and applied the same procedure as in K-means to attach ten clusters of categories to ten clusters of consumers. The result is shown in Table 4.5 (Again, to bedistinguishable, we named clusters as “Hcluster” here). 

```{r setup 4.8,  warning=FALSE, echo=FALSE}
socialmarketingraw=read.csv('https://raw.githubusercontent.com/jgscott/ECO395M/master/data/social_marketing.csv')
socialmarketing=filter(socialmarketingraw, socialmarketingraw$spam == 0 & socialmarketingraw$adult == 0)
X = socialmarketing[ , c(-1, -36, -37)]
X=X/rowSums(X)
Xs=scale(X, center = TRUE, scale = TRUE)

Xs_corr=cor(Xs)
d_Xscorr=dist(Xs_corr)

hc_corr1=hclust(d_Xscorr, method = "complete")

hc_corr10=cutree(hc_corr1, 10)

hc_corr10_table=data.frame(hc_corr10)
rownames=data.frame(rownames(hc_corr10_table))
hc_corr10_table=cbind(rownames, hc_corr10_table$hc_corr10)
colnames(hc_corr10_table)=c("Category", "Hcluster")
hc_corr10_table=hc_corr10_table[order(hc_corr10_table$Hcluster), ]
rownames(hc_corr10_table)=c()
#kable(hc_corr10_table,  format_caption = c("italic", "underline")) %>%
#  kable_styling(bootstrap_options = "striped", full_width = F)

Xclust10=X %>% transmute(c1=chatter+photo_sharing+shopping, c2=eco+current_events+uncategorized+home_and_garden+ music
                         +business+small_business, c3=travel+ politics+computers, c4=tv_film+ crafts +art, c5=sports_fandom
                         +food+family+religion+parenting+school, c6= news+automotive, c7= online_gaming+college_uni
                         +sports_playing, c8=health_nutrition+ outdoors+personal_fitness, c9=cooking+beauty +fashion,
                         c10=dating)
## kmeans
Xclust_scale10=scale(Xclust10, center = TRUE, scale = TRUE)
kmeans10=kmeans(Xclust_scale10, 10, nstart = 25)
cluster_k10=data.frame(kmeans10$cluster)
Xclustcombine10=cbind(Xclust10, cluster_k10)
colnames(Xclustcombine10)[11]="cluster"

result_save10=data.frame("Cluster"=character(10), "Consumers No."=numeric(10), "cluster in kmeans"=numeric(10),stringsAsFactors=FALSE)
for (i in 1:10){
  group=filter(Xclustcombine10, cluster==i)[, -11]
  gmean=data.frame(apply(group, 2, mean))
  result_save10[i,]=c(rownames(gmean)[apply(gmean, 2, which.max)], nrow(group), i)
}
result_save10=result_save10[order(result_save10$cluster), ]
rownames(result_save10)=c()
result_save10[which(result_save10$Consumers.No.==297) ,1]=colnames(Xclust_scale10)[10]

tableclust=data.frame()
for (j in c("c1", "c2", "c3", "c4", "c5", "c6", "c7", "c8", "c9", "c10")){
  a=result_save10[which(result_save10$Cluster==j), ]
  tableclust=rbind(tableclust,a)
}

Core_Categories=c("chatter, photo_sharing, shopping", "eco, current_events, uncategorized, home_and_garden, music, 
             business, small_business", "travel, politics, computers", "tv_film, crafts, art", "sports_fandom, food, family, religion, parenting, school", "news, automotive", "online_gaming, college_uni, sports_playing", "health_nutrition, outdoors, personal_fitness", "cooking, beauty, fashion", "dating" )

tableclust=cbind(tableclust, Core_Categories)[ ,-3]
tableclust=tableclust[order(tableclust[, 2], decreasing = TRUE),]

rownames(tableclust)=c("Hcluster 1", "Hcluster 2", "Hcluster 3", "Hcluster 4", "Hcluster 5", "Hcluster 6",
                       "Hcluster 7", "Hcluster 8", "Hcluster 9", "Hcluster 10")
tableclust=tableclust[,-1]

kable(tableclust, caption ="**Table 4.5: Result of Hierarachical Clustering with K-means**", format_caption = c("italic", "underline")) %>%
  kable_styling(bootstrap_options = "striped", full_width = F)
```

One good news is that there exists one to one mapping between ten clusters of categories and ten clusters of consumers. The other good news is that the distribution of the numbers of consumers over ten clusters is extremely close to the distribution in Table 4.3 or Table 4.4. Hence, by comparing the number of consumers, it is straightforward to link the clusters in Table 4.5 to those clusters in Table 4.3 or Table 4.4. The stable clustering structures across different approaches proves that our ten clusters are very robust based on our data.

It should be noted that the labels in Table 4.5 are supplements to categories of each cluster in Table 4.3 or Table 4.4. By labels in Table 4.5, the brand can try to do more accurate advertising or marketing for consumers in any clusters. It also helps the brand to recognize a new consumer. For example, if a new consumer has some crafts posts, just by categories in Table 4.3 or 4.4, it is difficulty to put her into any cluster. But by labels in Table 4.5, we know crafts is more likely to be clustered with movie and art, so it is reasonable to put her in Cluster 7 before having more information about this new consumer.

## Conclusion
Based on the above ten clusters of consumers, now we can make some comments.

The largest cluster of the audience of NurientH20 was those who focused much on health and workout. The second largest paid more attention to online socializing and shopping. The third to fifth largest clusters focused more on daily activities and business. All those largest five clusters probably could be grouped into a more general consumer type: less likely to be students, perhaps having families and children, caring about health and gym, possessing common hobbits like sports, fashion, and beauty. Also, they were more likely to have incomes. Therefore, this type of consumers could be in priority in the marketing of NutrientH20. 

The second type of consumers might be younger people. Most of them probably were college students with those classical student preferences like online gaming, sporting, watching movies and TV shows, dating and so forth. This type of consumers might have minor purchase power, but they could be potential future consumers of the brand, with more permanent brand loyalty. Hence, it was also important for NutrientH20 to attract those type of consumers as many as possible. 